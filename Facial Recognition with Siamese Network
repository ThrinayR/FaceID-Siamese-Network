{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fbe7e8c-c74e-46dd-8b2e-35ef99f825c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "TF: 2.20.0\n",
      "Devices: [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n"
     ]
    }
   ],
   "source": [
    "# Use %pip so it targets the current kernel\n",
    "%pip install -q tensorflow==2.20.0 opencv-python matplotlib\n",
    "\n",
    "import tensorflow as tf, cv2, matplotlib\n",
    "print(\"TF:\", tf.__version__)\n",
    "print(\"Devices:\", tf.config.list_physical_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b43737a-b43c-418b-86fb-4f3efbf86543",
   "metadata": {},
   "outputs": [],
   "source": [
    "#standard dependencies\n",
    "import cv2\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfb667c4-7522-4f03-9330-e44aef61a533",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tensorflow dependencies - Functional API\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Layer, Conv2D, Dense, MaxPooling2D, Input, Flatten\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bef64a0d-a340-49dd-ac60-1c15d70dde89",
   "metadata": {},
   "outputs": [],
   "source": [
    "POS_PATH = os.path.join('data','positive')\n",
    "NEG_PATH = os.path.join('data','negative')\n",
    "ANC_PATH = os.path.join('data', 'anchor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12bd73ff-03fa-4d77-a7a0-12e4012b2383",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(POS_PATH)\n",
    "os.makedirs(NEG_PATH)\n",
    "os.makedirs(ANC_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09c1cdd7-86e9-4f24-bb27-dca6a7113a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#http://vis-www.cs.umass.edu/lfw/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b49ea69-9cdc-4996-a34a-e7083f7ab6eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files moved successfully to: C:\\Users\\thrin\\FaceID\\data\\negative\n"
     ]
    }
   ],
   "source": [
    "SRC = r\"C:\\Users\\thrin\\FaceID\\lfw\"                  # source folder\n",
    "NEG_PATH = r\"C:\\Users\\thrin\\FaceID\\data\\negative\"   # destination folder\n",
    "\n",
    "# Make sure destination exists\n",
    "os.makedirs(NEG_PATH, exist_ok=True)\n",
    "\n",
    "for directory in os.listdir(SRC):\n",
    "    dir_path = os.path.join(SRC, directory)  # full path to subfolder\n",
    "    if os.path.isdir(dir_path):              # only process if it's a folder\n",
    "        for file in os.listdir(dir_path):    # <--- FIXED: only one argument here\n",
    "            EX_PATH = os.path.join(dir_path, file)\n",
    "            NEW_PATH = os.path.join(NEG_PATH, file)\n",
    "\n",
    "            # If file already exists in NEG_PATH, rename to avoid overwrite\n",
    "            if os.path.exists(NEW_PATH):\n",
    "                base, ext = os.path.splitext(file)\n",
    "                NEW_PATH = os.path.join(NEG_PATH, f\"{directory}_{base}{ext}\")\n",
    "\n",
    "            shutil.move(EX_PATH, NEW_PATH)\n",
    "\n",
    "print(f\"All files moved successfully to: {os.path.abspath(NEG_PATH)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5be0b31-0f58-4978-b722-e510d613d4ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data\\\\anchor\\\\82f1bd50-812f-11f0-92ed-4851c5aba03f.jpg'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.join(ANC_PATH, '{}.jpg'.format(uuid.uuid1()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b2a5649-24df-4d81-8e38-a93ce4c0ade6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Establish a connection to the webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "while cap.isOpened(): \n",
    "    ret, frame = cap.read()\n",
    "   \n",
    "    # Cut down frame to 250x250px\n",
    "    frame = frame[300:300+250,350:350+250, :]\n",
    "    \n",
    "    # Collect anchors \n",
    "    if cv2.waitKey(1) & 0XFF == ord('a'):\n",
    "        # Create the unique file path \n",
    "        imgname = os.path.join(r'C:\\Users\\thrin\\FaceID\\data\\anchor', '{}.jpg'.format(uuid.uuid1()))\n",
    "        # Write out anchor image\n",
    "        cv2.imwrite(imgname, frame)\n",
    "    \n",
    "    # Collect positives\n",
    "    if cv2.waitKey(1) & 0XFF == ord('p'):\n",
    "        # Create the unique file path \n",
    "        imgname = os.path.join(r'C:\\Users\\thrin\\FaceID\\data\\positive', '{}.jpg'.format(uuid.uuid1()))\n",
    "        # Write out positive image\n",
    "        cv2.imwrite(imgname, frame)\n",
    "    \n",
    "    # Show image back to screen\n",
    "    cv2.imshow('Image Collection', frame)\n",
    "    \n",
    "    # Breaking gracefully\n",
    "    if cv2.waitKey(1) & 0XFF == ord('q'):\n",
    "        break\n",
    "        \n",
    "# Release the webcam\n",
    "cap.release()\n",
    "# Close the image show frame\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "88a16170-e0fa-4102-a92d-f4815fa0f0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "\n",
    "# 1) EDIT these to your actual folders (raw strings or forward slashes)\n",
    "ANC_DIR = Path(r\"C:\\Users\\thrin\\FaceID\\data\\anchor\")\n",
    "POS_DIR = Path(r\"C:\\Users\\thrin\\FaceID\\data\\positive\")\n",
    "NEG_DIR = Path(r\"C:\\Users\\thrin\\FaceID\\data\\negative\")\n",
    "\n",
    "# 2) Helper to collect files safely (supports multiple extensions)\n",
    "EXTS = (\"jpg\", \"jpeg\", \"png\", \"bmp\", \"webp\")\n",
    "def collect(dir_path: Path):\n",
    "    files = []\n",
    "    for ext in EXTS:\n",
    "        files += list(dir_path.glob(f\"*.{ext}\"))\n",
    "    # Convert to POSIX strings for TF (avoids backslash issues)\n",
    "    files = [str(p.as_posix()) for p in files]\n",
    "    assert files, f\"No images found in {dir_path}\"\n",
    "    return tf.data.Dataset.from_tensor_slices(files)\n",
    "\n",
    "# 3) Build datasets (shuffle/take like before)\n",
    "anchor   = collect(ANC_DIR).shuffle(10000).take(300)\n",
    "positive = collect(POS_DIR).shuffle(10000).take(300)\n",
    "negative = collect(NEG_DIR).shuffle(10000).take(300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b03438ed-f6f0-45f6-9280-6ef4f64ac4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_test = anchor.as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c482b433-431d-40d2-8240-ec13786ed281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'C:/Users/thrin/FaceID/data/anchor/4c233ebb-80bc-11f0-ae09-4851c5aba03f.jpg'\n"
     ]
    }
   ],
   "source": [
    "print(dir_test.next())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b02c282b-d0ba-4278-81da-078da47fd7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(file_path):\n",
    "    \n",
    "    # Read in image from file path\n",
    "    byte_img = tf.io.read_file(file_path)\n",
    "    # Load in the image \n",
    "    img = tf.io.decode_jpeg(byte_img)\n",
    "    \n",
    "    # Preprocessing steps - resizing the image to be 100x100x3\n",
    "    img = tf.image.resize(img, (100,100))\n",
    "    # Scale image to be between 0 and 1 \n",
    "    img = img / 255.0\n",
    "    \n",
    "    # Return image\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eacf391b-063c-4da4-9bd6-e7a9ac95f26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = preprocess(r'C:\\Users\\thrin\\FaceID\\data\\anchor\\2a3e7306-80bc-11f0-abfd-4851c5aba03f.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cce044d6-38bb-4246-a89b-af55e77ab6bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float32(1.0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.numpy().max() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5879052c-654f-475f-9189-4b009643cb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "positives = tf.data.Dataset.zip((anchor, positive, tf.data.Dataset.from_tensor_slices(tf.ones(len(anchor)))))\n",
    "negatives = tf.data.Dataset.zip((anchor, negative, tf.data.Dataset.from_tensor_slices(tf.zeros(len(anchor)))))\n",
    "data = positives.concatenate(negatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e57b57-b725-4f44-851a-c54a73afbe38",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = data.as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd5151a-32a5-4dbe-be04-4cc5237ef8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = samples.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82781f7-4c8b-43f6-860d-37705486d189",
   "metadata": {},
   "outputs": [],
   "source": [
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9fe3bf-1d24-47c0-b426-1e2e29e0c578",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_twin(input_img, validation_img, label):\n",
    "    return(preprocess(input_img), preprocess(validation_img), label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd55468-1f5b-4d98-811c-bedfccb5b388",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = preprocess_twin(*example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b451c5-e658-4036-b70e-6cd90ff8565b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(res[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475feaf8-b95c-48e3-9711-6fc9d299a4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "res[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f1c24e-c870-4076-8184-a341e4935397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build dataloader pipeline\n",
    "data = data.map(preprocess_twin)\n",
    "data = data.cache()\n",
    "data = data.shuffle(buffer_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc062240-66b2-4c03-8d2e-7b1cd0b50bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training partition\n",
    "train_data = data.take(round(len(data)*.7))\n",
    "train_data = train_data.batch(16)\n",
    "train_data = train_data.prefetch(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e58ff1f-dd36-4e74-a85d-345996b1f389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing partition\n",
    "test_data = data.skip(round(len(data)*.7))\n",
    "test_data = test_data.take(round(len(data)*.3))\n",
    "test_data = test_data.batch(16)\n",
    "test_data = test_data.prefetch(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4292e4ed-4372-4697-9c32-7b45a1874fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Layer\n",
    "\n",
    "# ---- 1) Embedding model (same as before) ----\n",
    "def make_embedding():\n",
    "    inp = Input(shape=(100, 100, 3), name='input_image')\n",
    "    x = Conv2D(64, (10, 10), activation='relu')(inp)\n",
    "    x = MaxPooling2D(pool_size=(2, 2), padding='same')(x)\n",
    "    x = Conv2D(128, (7, 7), activation='relu')(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2), padding='same')(x)\n",
    "    x = Conv2D(128, (4, 4), activation='relu')(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2), padding='same')(x)\n",
    "    x = Conv2D(256, (4, 4), activation='relu')(x)\n",
    "    x = Flatten()(x)\n",
    "    z = Dense(4096, activation='sigmoid', name='embedding_vec')(x)\n",
    "    return Model(inp, z, name='embedding')\n",
    "\n",
    "embedding = make_embedding()\n",
    "\n",
    "# ---- 2) Two-argument L1 layer (Option B) ----\n",
    "class L1Dist(Layer):\n",
    "    def call(self, a, b):\n",
    "        return tf.math.abs(a - b)\n",
    "\n",
    "# IMPORTANT: instantiate the layer\n",
    "siamese_layer = L1Dist()\n",
    "\n",
    "# ---- 3) Build tensors and call correctly ----\n",
    "INPUT_SHAPE = (100, 100, 3)\n",
    "a_in = Input(shape=INPUT_SHAPE, name=\"anchor\")\n",
    "b_in = Input(shape=INPUT_SHAPE, name=\"candidate\")\n",
    "\n",
    "# Compute embeddings (ensure tensors, not lists)\n",
    "a_emb = embedding(a_in)\n",
    "b_emb = embedding(b_in)\n",
    "\n",
    "# Quick diagnostics\n",
    "print(\"a_emb type:\", type(a_emb), \"shape:\", getattr(a_emb, \"shape\", None))\n",
    "print(\"b_emb type:\", type(b_emb), \"shape:\", getattr(b_emb, \"shape\", None))\n",
    "\n",
    "# If some earlier code wrapped them in lists, unwrap safely:\n",
    "if isinstance(a_emb, (list, tuple)): a_emb = a_emb[0]\n",
    "if isinstance(b_emb, (list, tuple)): b_emb = b_emb[0]\n",
    "\n",
    "# ---- This is the line that must not error ----\n",
    "distances = siamese_layer(a_emb, b_emb)  # TWO args; no brackets\n",
    "\n",
    "# Optional head\n",
    "score = Dense(1, activation='sigmoid')(distances)\n",
    "siamese = Model([a_in, b_in], score)\n",
    "siamese.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8a9820-01f2-4cd3-af6c-8d606d5cc2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure instance + unwrap\n",
    "siamese_layer = L1Dist()\n",
    "if isinstance(inp_embedding, (list, tuple)): inp_embedding = inp_embedding[0]\n",
    "if isinstance(val_embedding, (list, tuple)): val_embedding = val_embedding[0]\n",
    "distances = siamese_layer(inp_embedding, val_embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f480e203-5f31-4a23-a19e-b36abccf7aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = Dense(1, activation='sigmoid')(distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc7b130-835f-451c-86e7-2b3771c889c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154319fe-1b7f-4e81-a52b-c5289801ceff",
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese_network = Model(inputs=[input_image, validation_image], outputs=classifier, name='SiameseNetwork')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd2bb25-d259-417e-b6ed-6fea059b2bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese_network.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a90b8f-2b1e-48f7-a504-c1ca5cc4bcea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_siamese_model(): \n",
    "    \n",
    "    # Anchor image input in the network\n",
    "    input_image = Input(name='input_img', shape=(100,100,3))\n",
    "    \n",
    "    # Validation image in the network \n",
    "    validation_image = Input(name='validation_img', shape=(100,100,3))\n",
    "    \n",
    "    # Combine siamese distance components\n",
    "    siamese_layer = L1Dist()\n",
    "    siamese_layer._name = 'distance'\n",
    "    distances = siamese_layer(embedding(input_image), embedding(validation_image))\n",
    "    \n",
    "    # Classification layer \n",
    "    classifier = Dense(1, activation='sigmoid')(distances)\n",
    "    \n",
    "    return Model(inputs=[input_image, validation_image], outputs=classifier, name='SiameseNetwork')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0f43b6-01db-4fce-845f-cf1cabbd87b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese_model = make_siamese_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6776cf41-1658-4d7c-8ba0-9ff50edd50ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9c0af6-fec3-4787-8468-4b1b97650dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_cross_loss = tf.losses.BinaryCrossentropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb595d9b-cda6-4c83-84fd-d332478c3224",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.Adam(1e-4) # 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232b5a9d-55ef-4be2-b3d0-9aa51647a1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt')\n",
    "checkpoint = tf.train.Checkpoint(opt=opt, siamese_model=siamese_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c653ca65-bd41-4c0b-9bc5-393b1677b997",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch = train_data.as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc6bc41-62c0-4768-887f-1a575adcc0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_1 = test_batch.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1971aa-3fa5-41c0-8e72-26b227890af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = batch_1[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca62be1-43f9-4a0e-a68b-79affc0efbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = batch_1[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500eabb3-5fa0-435b-8467-5d48b75ef08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369bf897-da16-4c5c-a219-549c0db44ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.losses.BinaryCrossentropy??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f0a998-aece-4d6b-9949-4b40ca73747b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(batch):\n",
    "    \n",
    "    # Record all of our operations \n",
    "    with tf.GradientTape() as tape:     \n",
    "        # Get anchor and positive/negative image\n",
    "        X = batch[:2]\n",
    "        # Get label\n",
    "        y = batch[2]\n",
    "        \n",
    "        # Forward pass\n",
    "        yhat = siamese_model(X, training=True)\n",
    "        # Calculate loss\n",
    "        loss = binary_cross_loss(y, yhat)\n",
    "    print(loss)\n",
    "        \n",
    "    # Calculate gradients\n",
    "    grad = tape.gradient(loss, siamese_model.trainable_variables)\n",
    "    \n",
    "    # Calculate updated weights and apply to siamese model\n",
    "    opt.apply_gradients(zip(grad, siamese_model.trainable_variables))\n",
    "    \n",
    "    # Return loss\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210a04d1-f1b1-43de-bd83-e4dc7bbac41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data, EPOCHS):\n",
    "    # Loop through epochs\n",
    "    for epoch in range(1, EPOCHS+1):\n",
    "        print('\\n Epoch {}/{}'.format(epoch, EPOCHS))\n",
    "        progbar = tf.keras.utils.Progbar(len(data))\n",
    "        \n",
    "        # Loop through each batch\n",
    "        for idx, batch in enumerate(data):\n",
    "            # Run train step here\n",
    "            train_step(batch)\n",
    "            progbar.update(idx+1)\n",
    "        \n",
    "        # Save checkpoints\n",
    "        if epoch % 10 == 0: \n",
    "            checkpoint.save(file_prefix=checkpoint_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92368fdf-81d6-48c2-a3ce-2bc99462f975",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08af1326-902f-4138-a01c-427068838d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(train_data, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4013d62b-a0a6-4c78-80db-95c31a1a9823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import metric calculations\n",
    "from tensorflow.keras.metrics import Precision, Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3fde48-c758-4a9e-b633-e9bbd83e0e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get a batch of test data\n",
    "test_input, test_val, y_true = test_data.as_numpy_iterator().next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba0b776-d25a-4766-8590-cd168b8678fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Make predictions\n",
    "y_hat = siamese_model.predict([test_input, test_val])\n",
    "y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3180f100-db3a-4485-94e4-9e0e1900edbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Post processing the results \n",
    "[1 if prediction > 0.5 else 0 for prediction in y_hat ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1559ac-6ced-4a4f-84e2-cb961aa9d3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0d6d04-a4cb-406d-b272-1acd6121da85",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Creating a metric object \n",
    "m = Recall()\n",
    "\n",
    "# Calculating the recall value \n",
    "m.update_state(y_true, y_hat)\n",
    "\n",
    "# Return Recall Result\n",
    "m.result().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3eb1ae-db50-4417-8299-f5109b036560",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Creating a metric object \n",
    "m = Precision()\n",
    "\n",
    "# Calculating the recall value \n",
    "m.update_state(y_true, y_hat)\n",
    "\n",
    "# Return Recall Result\n",
    "m.result().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b168cf-3a3f-403f-9261-ab9d144914a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set plot size \n",
    "plt.figure(figsize=(10,8))\n",
    "\n",
    "# Set first subplot\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(test_input[0])\n",
    "\n",
    "# Set second subplot\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(test_val[0])\n",
    "\n",
    "# Renders cleanly\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2aad38-d790-462c-95b6-a4a67f36851b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save weights\n",
    "siamese_model.save('siamesemodel.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f887cd31-e3e8-41fc-978b-9fe918dc12a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "L1Dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9f2d59-fe03-452e-afd5-31d28a0bae45",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q scikit-learn pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "098992d2-9e8f-493c-8418-2e5d9ecbfaec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC = 0.6405\n",
      "EER = 0.3800\n",
      "TPR@FPR=0.0010 = 0.0000  (threshold=inf)\n"
     ]
    }
   ],
   "source": [
    "# --- config: change these paths/names ---\n",
    "MODEL_PATH = r\"C:\\Users\\thrin\\FaceID\\siamesemodel.keras\"\n",
    "ANCHOR_DIR = r\"C:\\Users\\thrin\\FaceID\\data\\anchor\" \n",
    "POS_DIR    = r\"C:\\Users\\thrin\\FaceID\\data\\positive\"         \n",
    "NEG_DIR    = r\"C:\\Users\\thrin\\FaceID\\data\\negative\"          \n",
    "IMG_SIZE   = (100, 100)                         \n",
    "N_POS_PAIRS = 300                               # how many genuine pairs to sample\n",
    "N_NEG_PAIRS = 300                               # how many impostor pairs to sample\n",
    "TARGET_FPR  = 1e-3                               # 0.1% false positives (tunable)\n",
    "\n",
    "import os, random, numpy as np, tensorflow as tf\n",
    "from glob import glob\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from PIL import Image\n",
    "\n",
    "# ----- util -----\n",
    "def load_img(p, size):\n",
    "    img = Image.open(p).convert(\"RGB\").resize(size)\n",
    "    x = np.asarray(img, dtype=np.float32)/255.0\n",
    "    return x\n",
    "\n",
    "def list_images(d):\n",
    "    exts = (\".jpg\",\".jpeg\",\".png\",\".bmp\",\".webp\")\n",
    "    return [p for p in glob(os.path.join(d, \"*\")) if p.lower().endswith(exts)]\n",
    "\n",
    "# ----- data: build pairs from your layout -----\n",
    "A = list_images(ANCHOR_DIR)\n",
    "P = list_images(POS_DIR)\n",
    "N = list_images(NEG_DIR)\n",
    "assert len(A)>0 and len(P)>0 and len(N)>0, \"Check your folders have images\"\n",
    "\n",
    "rng = random.Random(1337)\n",
    "def sample_pos_pairs(k):\n",
    "    pairs = []\n",
    "    for _ in range(k):\n",
    "        a = rng.choice(A + P)   # both are \"you\"\n",
    "        b = rng.choice(A + P)\n",
    "        while b == a:\n",
    "            b = rng.choice(A + P)\n",
    "        pairs.append((a,b,1))\n",
    "    return pairs\n",
    "\n",
    "def sample_neg_pairs(k):\n",
    "    pairs = []\n",
    "    AP = A + P\n",
    "    for _ in range(k):\n",
    "        a = rng.choice(AP)\n",
    "        b = rng.choice(N)\n",
    "        pairs.append((a,b,0))\n",
    "    return pairs\n",
    "\n",
    "pairs = sample_pos_pairs(N_POS_PAIRS) + sample_neg_pairs(N_NEG_PAIRS)\n",
    "rng.shuffle(pairs)\n",
    "\n",
    "# ----- model loader (supports both pairwise or embedding types) -----\n",
    "# If you saved with a custom L1/L2 layer, add it here:\n",
    "class L1Dist(tf.keras.layers.Layer):\n",
    "    def call(self, a, b): return tf.math.abs(a - b)\n",
    "\n",
    "custom_objects = {\"L1Dist\": L1Dist}\n",
    "\n",
    "model = None\n",
    "if \"model\" in globals() and isinstance(model, tf.keras.Model):\n",
    "    M = model\n",
    "elif \"siamese\" in globals() and isinstance(siamese, tf.keras.Model):\n",
    "    M = siamese\n",
    "else:\n",
    "    M = tf.keras.models.load_model(MODEL_PATH, custom_objects=custom_objects)\n",
    "\n",
    "# Detect architecture: 2 inputs → pairwise classifier, else → embedding\n",
    "is_pairwise = len(M.inputs) == 2 and (M.output_shape[-1] == 1)\n",
    "\n",
    "# ----- scoring -----\n",
    "def to_batches(seq, bs=64):\n",
    "    for i in range(0,len(seq),bs):\n",
    "        yield seq[i:i+bs]\n",
    "\n",
    "scores, labels = [], []\n",
    "if is_pairwise:\n",
    "    # Pairwise classifier: model([A,B]) returns match probability\n",
    "    for batch in to_batches(pairs, 64):\n",
    "        a_paths, b_paths, y = zip(*batch)\n",
    "        Aarr = np.stack([load_img(p, IMG_SIZE) for p in a_paths], axis=0)\n",
    "        Barr = np.stack([load_img(p, IMG_SIZE) for p in b_paths], axis=0)\n",
    "        s = M.predict([Aarr, Barr], verbose=0).ravel()\n",
    "        scores.extend(s.tolist()); labels.extend(y)\n",
    "else:\n",
    "    # Embedding model: compute cosine similarity\n",
    "    def embed(paths):\n",
    "        X = np.stack([load_img(p, IMG_SIZE) for p in paths], axis=0)\n",
    "        Z = M.predict(X, verbose=0)\n",
    "        Z = Z / (np.linalg.norm(Z, axis=1, keepdims=True) + 1e-9)\n",
    "        return Z\n",
    "    # Do in chunks to save RAM\n",
    "    CHUNK = 512\n",
    "    for chunk in to_batches(pairs, CHUNK):\n",
    "        a_paths, b_paths, y = zip(*chunk)\n",
    "        Za, Zb = embed(a_paths), embed(b_paths)\n",
    "        sim = np.sum(Za*Zb, axis=1)  # cosine similarity ∈ [-1,1]\n",
    "        # Convert to \"match score\" in [0,1] (higher=more likely same)\n",
    "        s = (sim + 1.0)/2.0\n",
    "        scores.extend(s.tolist()); labels.extend(y)\n",
    "\n",
    "scores = np.array(scores, dtype=float)\n",
    "labels = np.array(labels, dtype=int)\n",
    "\n",
    "# ----- metrics: ROC/AUC, EER, threshold at target FPR -----\n",
    "fpr, tpr, thr = roc_curve(labels, scores)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "fnr = 1 - tpr\n",
    "eer_idx = np.nanargmin(np.abs(fnr - fpr))\n",
    "eer = (fnr[eer_idx] + fpr[eer_idx]) / 2\n",
    "# Threshold for the FPR you care about\n",
    "idx = np.argmin(np.abs(fpr - TARGET_FPR))\n",
    "thresh = thr[idx]\n",
    "print(f\"AUC = {roc_auc:.4f}\")\n",
    "print(f\"EER = {eer:.4f}\")\n",
    "print(f\"TPR@FPR={TARGET_FPR:.4f} = {tpr[idx]:.4f}  (threshold={thresh:.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "df10c975-d7c1-45cb-b98d-ae7121b09b5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FACES_ROOT: C:/Users/thrin/FaceID/data/faces\n",
      "me images: 766\n",
      "identities: 5750\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "# --- EDIT THESE THREE ---\n",
    "ANCHOR_DIR = r\"C:\\Users\\thrin\\FaceID\\data\\anchor\"\n",
    "POS_DIR    = r\"C:\\Users\\thrin\\FaceID\\data\\positive\"\n",
    "NEG_DIR    = r\"C:\\Users\\thrin\\FaceID\\data\\negative\"\n",
    "# ------------------------\n",
    "\n",
    "# Force-convert to Path objects (even if previously defined as strings)\n",
    "ANCHOR_DIR = Path(ANCHOR_DIR)\n",
    "POS_DIR    = Path(POS_DIR)\n",
    "NEG_DIR    = Path(NEG_DIR)\n",
    "\n",
    "# Derive faces root next to your current data folder\n",
    "DATA_ROOT  = ANCHOR_DIR.parent            # e.g., ...\\FaceID\\data\n",
    "FACES_ROOT = DATA_ROOT / \"faces\"\n",
    "FACES_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 1) Put all your images (anchor+positive) into faces/me\n",
    "ME = FACES_ROOT / \"me\"\n",
    "ME.mkdir(exist_ok=True)\n",
    "for p in list(ANCHOR_DIR.glob(\"*\")) + list(POS_DIR.glob(\"*\")):\n",
    "    if p.is_file():\n",
    "        shutil.copy2(p, ME / p.name)\n",
    "\n",
    "# 2) Split LFW negatives into faces/<person_name>/ using filename like \"Aaron_Eckhart_0001.jpg\"\n",
    "for f in NEG_DIR.glob(\"*\"):\n",
    "    if not f.is_file():\n",
    "        continue\n",
    "    stem = f.stem  # e.g., \"Aaron_Eckhart_0001\"\n",
    "    if \"_\" not in stem:\n",
    "        continue\n",
    "    person = stem.rsplit(\"_\", 1)[0]  # -> \"Aaron_Eckhart\"\n",
    "    dest = FACES_ROOT / person\n",
    "    dest.mkdir(exist_ok=True)\n",
    "    shutil.copy2(f, dest / f.name)\n",
    "\n",
    "# Report\n",
    "print(\"FACES_ROOT:\", FACES_ROOT.as_posix())\n",
    "print(\"me images:\", len(list((FACES_ROOT/'me').glob('*'))))\n",
    "print(\"identities:\", sum(1 for d in FACES_ROOT.iterdir() if d.is_dir()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d354644-95ac-4cc3-951c-8d46a2c577e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload model \n",
    "model = tf.keras.models.load_model('siamesemodel.h5', \n",
    "                                   custom_objects={'L1Dist':L1Dist, 'BinaryCrossentropy':tf.losses.BinaryCrossentropy})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be8c165-82fe-47c1-81eb-acaf5e466a4e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Make predictions with reloaded model\n",
    "model.predict([test_input, test_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9914a5de-7013-49db-9c06-f292ee5ddd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337db9a2-210d-405f-914f-e561461cdbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(os.path.join(r'C:\\Users\\thrin\\FaceID\\data\\application data', r'C:\\Users\\thrin\\FaceID\\data\\application data\\verification images'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afd6157-1160-42d3-baf4-18643f0e62d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.join(r'C:\\Users\\thrin\\FaceID\\data\\application data', r'C:\\Users\\thrin\\FaceID\\data\\application data\\verification images', 'input_image.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9594d7b-e1ed-43f7-b6ba-998a6ce6ed40",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for image in os.listdir(os.path.join(r'C:\\Users\\thrin\\FaceID\\data\\application data', r'C:\\Users\\thrin\\FaceID\\data\\application data\\verification images')):\n",
    "    validation_img = os.path.join(r'C:\\Users\\thrin\\FaceID\\data\\application data', r'C:\\Users\\thrin\\FaceID\\data\\application data\\verification images', image)\n",
    "    print(validation_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2288ba-779e-4368-bf42-caf49a08fcf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify(model, detection_threshold, verification_threshold):\n",
    "    # Build results array\n",
    "    results = []\n",
    "    for image in os.listdir(os.path.join(r'C:\\Users\\thrin\\FaceID\\data\\application data', r'C:\\Users\\thrin\\FaceID\\data\\application data\\verification images')):\n",
    "        input_img = preprocess(os.path.join(r'C:\\Users\\thrin\\FaceID\\data\\application data', r'C:\\Users\\thrin\\FaceID\\data\\application data\\verification images', 'input_image.jpg'))\n",
    "        validation_img = preprocess(os.path.join(r'C:\\Users\\thrin\\FaceID\\data\\application data', r'C:\\Users\\thrin\\FaceID\\data\\application data\\verification images', image))\n",
    "        \n",
    "        # Make Predictions \n",
    "        result = model.predict(list(np.expand_dims([input_img, validation_img], axis=1)))\n",
    "        results.append(result)\n",
    "    \n",
    "    # Detection Threshold: Metric above which a prediciton is considered positive \n",
    "    detection = np.sum(np.array(results) > detection_threshold)\n",
    "    \n",
    "    # Verification Threshold: Proportion of positive predictions / total positive samples \n",
    "    verification = detection / len(os.listdir(os.path.join(r'C:\\Users\\thrin\\FaceID\\data\\application data', r'C:\\Users\\thrin\\FaceID\\data\\application data\\verification images'))) \n",
    "    verified = verification > verification_threshold\n",
    "    \n",
    "    return results, verified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a606fe-9d9b-4487-b1a9-d95791575b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    frame = frame[300:300+250,350:350+250, :]\n",
    "    \n",
    "    cv2.imshow('Verification', frame)\n",
    "    \n",
    "    # Verification trigger\n",
    "    if cv2.waitKey(10) & 0xFF == ord('v'):\n",
    "        # Save input image to application_data/input_image folder \n",
    "        cv2.imwrite(os.path.join(r'C:\\Users\\thrin\\FaceID\\data\\application data', r'C:\\Users\\thrin\\FaceID\\data\\application data\\verification images', 'input_image.jpg'), frame)\n",
    "        # Run verification\n",
    "        results, verified = verify(model, 0.9, 0.7)\n",
    "        print(verified)\n",
    "    \n",
    "    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8bdd152b-6de8-4069-933e-d4f47c11de1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_inputs: 2 output_shape: (None, 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# If you used a custom L1 layer:\n",
    "class L1Dist(tf.keras.layers.Layer):\n",
    "    def call(self, a, b): return tf.math.abs(a - b)\n",
    "\n",
    "MODEL_PATH = r\"C:\\Users\\thrin\\FaceID\\siamesemodel.keras\"\n",
    "\n",
    "M = tf.keras.models.load_model(MODEL_PATH, custom_objects={\"L1Dist\": L1Dist})\n",
    "print(\"num_inputs:\", len(M.inputs), \"output_shape:\", M.output_shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0f11e2b3-b454-4433-9673-db32b22c8b26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: C:\\Users\\thrin\\FaceID\\app_fastapi.py | bytes: 2049\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "APP_CODE = r'''\n",
    "from fastapi import FastAPI, UploadFile, File, Query\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "class L1Dist(tf.keras.layers.Layer):\n",
    "    def call(self, a, b): return tf.math.abs(a - b)\n",
    "\n",
    "custom_objects = {\"L1Dist\": L1Dist}\n",
    "\n",
    "ROOT = Path(__file__).parent\n",
    "MODEL_PATH = ROOT / \"siamesemodel.keras\"  # change if your file name differs\n",
    "\n",
    "M = tf.keras.models.load_model(MODEL_PATH.as_posix(), custom_objects=custom_objects)\n",
    "\n",
    "IS_PAIRWISE = (len(M.inputs) == 2 and M.output_shape[-1] == 1)\n",
    "try:\n",
    "    H = int(M.inputs[0].shape[1]); W = int(M.inputs[0].shape[2])\n",
    "except Exception:\n",
    "    H, W = 100, 100\n",
    "\n",
    "app = FastAPI(title=\"Face Verification API\")\n",
    "\n",
    "def decode_and_resize(img_bytes, size):\n",
    "    x = tf.image.decode_image(img_bytes, channels=3, expand_animations=False)\n",
    "    x = tf.image.resize(x, size)\n",
    "    x = tf.cast(x, tf.float32) / 255.0\n",
    "    return x\n",
    "\n",
    "def embed_one(img_bytes):\n",
    "    x = decode_and_resize(img_bytes, (H, W))[None, ...]\n",
    "    z = M(x, training=False).numpy()[0]\n",
    "    z = z / (np.linalg.norm(z) + 1e-9)\n",
    "    return z\n",
    "\n",
    "@app.get(\"/\")\n",
    "def info():\n",
    "    return {\"model\": MODEL_PATH.name, \"pairwise_classifier\": IS_PAIRWISE, \"input_size\": [H, W]}\n",
    "\n",
    "@app.post(\"/verify\")\n",
    "async def verify(\n",
    "    file_a: UploadFile = File(...),\n",
    "    file_b: UploadFile = File(...),\n",
    "    threshold: float = Query(0.8, description=\"Use your calibrated value after Step 6\")\n",
    "):\n",
    "    a_bytes = await file_a.read()\n",
    "    b_bytes = await file_b.read()\n",
    "\n",
    "    if IS_PAIRWISE:\n",
    "        xa = decode_and_resize(a_bytes, (H, W))[None, ...]\n",
    "        xb = decode_and_resize(b_bytes, (H, W))[None, ...]\n",
    "        prob = float(M.predict([xa, xb], verbose=0).ravel()[0])\n",
    "        return {\"type\": \"pairwise\", \"score\": prob, \"match\": prob >= threshold, \"threshold\": threshold}\n",
    "    else:\n",
    "        ea = embed_one(a_bytes)\n",
    "        eb = embed_one(b_bytes)\n",
    "        sim = float(np.dot(ea, eb))\n",
    "        score = (sim + 1.0) / 2.0\n",
    "        return {\"type\": \"embedding\", \"score\": score, \"match\": score >= threshold, \"threshold\": threshold}\n",
    "'''\n",
    "\n",
    "target = Path(r\"C:\\Users\\thrin\\FaceID\\app_fastapi.py\")\n",
    "target.write_text(APP_CODE, encoding=\"utf-8\")\n",
    "print(\"Wrote:\", target, \"| bytes:\", target.stat().st_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0f046907-1c6d-482e-8f88-6b6705d7fd99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thrin\\AppData\\Local\\Programs\\Python\\Python313\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys; print(sys.executable)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
